{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9BCFtiX_x9dO","colab_type":"code","colab":{}},"source":["from .finding_center import find_center\n","\n","import torch\n","import torch.optim as optim\n","\n","import time\n","import logging"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxVMWgTpzaji","colab_type":"code","colab":{}},"source":["def train(trainloader, classifier, device='cuda', lr=1e-3, weight_decay=1e-6, \n","          milestones=[20, 40, 60], c=None, R=None, nu=None, warm_up_n_epochs=None, \n","          clf_epochs=100, accumulation_steps=32, objective='one-class', accumulate=False):\n","    logger = logging.getLogger()\n","\n","    # Set device for network\n","    classifier = classifier.to(device)\n","\n","    # Set optimizer (Adam optimizer for now)\n","    optimizer = optim.Adam(classifier.parameters(), lr=lr, weight_decay=weight_decay,\n","                           amsgrad=True)\n","\n","    # Set learning rate scheduler\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n","\n","    # Initialize hypersphere center c (if c not loaded)\n","    if c is None:\n","        logger.info('Initializing center c...')\n","        c = find_center(trainloader, classifier)\n","        logger.info('Center c initialized.')\n","\n","    # Training\n","    logger.info('Starting training...')\n","    start_time = time.time()\n","    classifier.train()\n","    for epoch in range(clf_epochs):\n","\n","        scheduler.step()\n","        if epoch in milestones:\n","            logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n","\n","        loss_epoch = 0.0\n","        n_batches = 0\n","        epoch_start_time = time.time()\n","        for i, data in enumerate(trainloader):\n","            inputs, _, _ = data\n","            inputs = inputs.to(device)\n","\n","            # Zero the network parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Update network parameters via backpropagation: forward + backward + optimize\n","            outputs = classifier(inputs)\n","            dist = torch.sum((outputs - c) ** 2, dim=1)\n","            if objective == 'soft-boundary':\n","                scores = dist - R ** 2\n","                loss = R ** 2 + (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n","            else:\n","                loss = torch.mean(dist)\n","            if accumulate:\n","                loss = loss / accumulation_steps\n","                loss.backward()\n","                if (i + 1) % accumulation_steps == 0:\n","                    optimizer.step()\n","                    classifier.zero_grad()\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","            loss_epoch += loss.item()\n","            n_batches += 1\n","\n","            # Update hypersphere radius R on mini-batch distances\n","            if (objective == 'soft-boundary') and (epoch >= warm_up_n_epochs):\n","                R = torch.tensor(get_radius(dist, nu), device=device)\n","\n","        # log epoch statistics\n","        epoch_train_time = time.time() - epoch_start_time\n","        logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n","                    .format(epoch + 1, clf_epochs, epoch_train_time, loss_epoch / n_batches))\n","\n","    train_time = time.time() - start_time\n","    logger.info('Training time: %.3f' % train_time)\n","\n","    logger.info('Finished training.')\n","\n","    return classifier"],"execution_count":0,"outputs":[]}]}