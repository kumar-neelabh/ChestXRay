{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pretraining.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KeDDPpN-YmIY","colab_type":"code","colab":{}},"source":["import torch\n","import logging\n","import time\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PX5aIvVhToZS","colab_type":"code","colab":{}},"source":["def pretrain(trainloader, autoencoder, device='cuda', lr=1e-3, \n","             milestones=[20, 40, 60], ae_epochs=100, accumulation_steps=64,\n","             weight_decay=1e-6, accumulate=False):\n","    \n","    logger = logging.getLogger()\n","\n","    # Set device for network\n","    autoencoder = autoencoder.to(device)\n","\n","    # Set optimizer (Adam optimizer for now)\n","    optimizer = optim.Adam(autoencoder.parameters(), lr=lr, \n","                           weight_decay=weight_decay, amsgrad=True)\n","\n","    # Set learning rate scheduler\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n","    \n","    if not accumulate:\n","        accumulation_steps = 0\n","    # Training\n","    logger.info('Starting pretraining...')\n","    logger.info('Learning rate : {}'.format(lr))\n","    logger.info('Gradient accumulation : {}'.format(accumulate))\n","    logger.info('Accumulation steps : {}'.format(accumulation_steps))\n","    logger.info('AE epochs : {}'.format(ae_epochs))\n","    \n","    start_time = time.time()\n","    autoencoder.train()\n","\n","    for epoch in range(ae_epochs):\n","        scheduler.step()\n","        if epoch in milestones:\n","            logger.info('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n","\n","        loss_epoch = 0.0\n","        n_batches = 0\n","        epoch_start_time = time.time()\n","        for i, data in enumerate(trainloader):\n","            inputs, _, _ = data\n","            inputs = inputs.to(device)\n","\n","            # Zero the network parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Update network parameters via backpropagation: forward + backward + optimize\n","            outputs = autoencoder(inputs)\n","            scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n","            loss = torch.mean(scores)\n","            if accumulate:\n","                loss = loss / accumulation_steps\n","                loss.backward()\n","                if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n","                    optimizer.step()                            # Now we can do an optimizer step\n","                    autoencoder.zero_grad()                           # Reset gradients tensors\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","\n","            loss_epoch += loss.item()\n","            n_batches += 1\n","\n","        # log epoch statistics\n","        epoch_train_time = time.time() - epoch_start_time\n","        logger.info('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n","                    .format(epoch + 1, ae_epochs, epoch_train_time, loss_epoch / n_batches))\n","\n","    pretrain_time = time.time() - start_time\n","    logger.info('Pretraining time: %.3f' % pretrain_time)\n","    logger.info('Finished pretraining.')\n","\n","    return autoencoder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZRY2cP5DLMV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}